# [Machine-Learning-Projects-for-2023](https://github.com/sbsahane12/Machine-Learning-Projects-for-2023.git)

# [Project 1: House Price Prediction Using Advanced Machine Learning](https://github.com/sbsahane12/House-Price-Prediction-Using-Advanced-Machine-Learning.git)
This is a project I did for my masters research paper, where I build a House Price Prediction system for a House dataset.
## Install
This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)

## Models trained on: 
1. Linear Regression Algorithm 
2. Logistic Regression Algorithm 
3. Decision Tree
4. SVM
5. NaÃ¯ve Bayes
6. KNN
7. K-Means Clustering
8. Random Forest

# [Project 2: Air-quality-prediction Using Advanced Machine Learning](https://github.com/sbsahane12/Air-quality-prediction-Using-Advanced-Machine-Learning.git) 
As air pollution is a complex mixture of toxic components with considerable impact on humans, forecasting air pollution concentration emerges as a priority for improving life quality. So with the help of Python tools and some Machine Learning algorithms, we try to predict the air quality. 

## Aim :
### In this project we will be building an Air Quality Index Predictor with the help of Machine Learning Models and Auto ML library i.e. TPOT

## Install
This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)

You will also need to have software installed to run and execute a [Jupyter Notebook](http://ipython.org/notebook.html)

If you do not have Python installed yet, it is highly recommended that you install the [Anaconda](http://continuum.io/downloads) distribution of Python, which already has the above packages and more included.


## Models trained on: 
1. Linear Regression
2. Xgboost Regressor
3. Random Forest Regressor

****And doing the Hyperameter tuning for the above****

# [Project  3: Ecom-Market-Mix-Modle Using ML](https://github.com/sbsahane12/Ecom-Market-Mix-Modle-Using-ML.git) 
To build a MMM for ElecKart Ontario based ecommerce company
## Problem Statement  
### Background  - Business Understanding   
ElecKart is an e-commerce firm based out of Ontario, Canada specialising in electronic products. Over the last year, they had spent a significant amount of money on marketing. Occasionally, they had also offered big-ticket promotions (similar to the Big Billion Day). They are about to create a marketing budget for the next year, which includes spending on commercials, online campaigns, and pricing & promotion strategies. The CFO feels that the money spent over the last 12 months on marketing was not sufficiently impactful, and, that they can either cut on the budget or reallocate it optimally across marketing levers to improve the revenue response.  
## Install
This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)
## Data Understanding
You have to use the data from July 2015 to June 2016. The data consists of the following types of information:  
   
Order level data  
â€¢	FSN ID: The unique identification of each SKU  
â€¢	Order Date: Date on which the order was placed  
â€¢	Order ID: The unique identification number of each order  
â€¢	Order item ID: Suppose you order 2 different products under the same order, it generates 2 different order Item IDs under the same order ID; orders are tracked by the Order Item ID.  
â€¢	GMV: Gross Merchandise Value or Revenue  
â€¢	Units: Number of units of the specific product sold  
â€¢	Order payment type: How the order was paid â€“ prepaid or cash on delivery  
â€¢	SLA: Number of days it typically takes to deliver the product  
â€¢	Cust id: Unique identification of a customer  
â€¢	Product MRP: Maximum retail price of the product  
â€¢	Product procurement SLA: Time typically taken to procure the product Apart from this, the following information is also available:  
â€¢	Monthly spend on various advertising channels  
â€¢	Days when there was any special sale  
â€¢	Monthly NPS score â€“ this may work as a proxy to â€˜voice of the customerâ€™  
â€¢	Stock Index of the company on a monthly basis   

## Data Preparation  
You have to create market mix models for three product subcategories  - camera accessory, home audio and gaming accessory. Also, the models have to be built at a weekly level for each of the sub-categories.  
## Models trained on: 
1. Linear Regression
2. Xgboost Regressor

# [Project 4:Telecom Churn Logistic Regression with PCA](https://github.com/sbsahane12/Telecom-Churn-Logistic-Regression-with-PCA.git)
## Telecom Churn: Logistic Regression with PCA
With 21 predictor variables, we need to predict whether a particular customer will switch to another telecom provider or not. In telecom terminology, customer attrition is referred to as 'churn'.
## Install
This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)

## Models trained on: 
1. PCA Algorithm 
2. Logistic Regression Algorithm 

# [Project  5: Countries  Clustering  Assignment Using ML](https://github.com/sbsahane12/Countries-Clustering-Assignment-Using-ML.git) 
## Problem Statement
HELP International is an international humanitarian NGO that is committed to fighting poverty and  providing the people of backward countries with basic amenities and relief during the time of  disasters and natural calamities
After the recent funding programmes, they have been able to raise around $ 10 million. Now the  CEO of the NGO needs to decide how to use this money strategically and effectively.
The significant issues that come while making this decision are mostly related to choosing the
countries that are in the direst need of aid.
As a Data Scientist, we need to find the countries in direst need and help CEO of HELP  International in using the fund money to reach right countries
## Problem Approach
As we have the Data of countries like child mortality rate, GDP Per Capita, Income etc. , we can
use Clustering to segregate the countries into different groups
Steps :
Data Inspection â€“ Missing Values if any, EDA
Outlier Analysis
Data Pre-processing
Finding Optimal number of Clusters
Modelling
KMeans Clustering
Hierarchical Clustering â€“ Single and Complete Linkages
Listing down top 5 countries in need
## Install
This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)
## Result
- From the list of poor countries we obtained, sorted the list on income, gdpp, child_mortality rate.
- Top 5 countries which are in direst need :
- Congo, Dem Rep
- Liberia
- Burundi
- Niger
- Central African Republic

## Models trained on: 
#### Built the model using â€œEuclidean distanceâ€ as metric and linkage type as â€œSingleâ€
- Plotted the Dendrogram for single linkage, we wonâ€™t be able to observe good clusters in single linkage
- Built the model using Complete Linkage, we could clearly observe 3 clusters formed. Used Cut_tree with n_clusters = 3 to get the labels of the clusters formed





# [Project  6: FineTech App Using ML](https://github.com/sbsahane12/-FineTech-App-Using-ML.git) 
## Problem Statement
- Follow the â€œDirecting Customers to Subscription Through Financial App Behavior Analysis Machine Learning End to End Projectâ€ step by step to get 4 Bonus.
- We are finding how much time the customer takes to get enrolled in the premium feature app after registration. For that subtract â€˜fineTech_appData.first_openâ€™ from â€˜fineTech_appData.enrolled_dateâ€™ and set data type as timedelta64 in hours.
## Problem Approach
- 1)We saw the heatmap correlation matrix but this was not showing correlation clearly but you can easily understand which feature is how much correlated with â€˜enrolledâ€™ feature using the above barplot.     
- 2)The â€˜numscreensâ€™ and â€˜minigameâ€™ is strongly positively correlated with â€˜enrolledâ€™ feature than other feature. 
- 3)The â€˜hourâ€™, â€˜ageâ€™ and â€˜used_premium_featureâ€™ are strongly negatively correlated with the â€˜enrolledâ€™ feature.
## Steps :
- Data Inspection â€“ Missing Values if any, EDA
- Outlier Analysis
- Data Pre-processing
- Finding Optimal number of Clusters
- Modelling
- KMeans Clustering
- Hierarchical Clustering â€“ Single and Complete Linkages
- Listing down top 5 countries in need
## Install
This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)
- 
## Machine Learning Model Building
- The target variable is categorical type 0 and 1, so we have to use supervised classification algorithms.To build the best model, we have to train and test the dataset with multiple Machine Learning algorithms then we can find the best ML model. So letâ€™s try.First, we import the required packages.





# [Project  7: Handwriting digit recognition using SVM](https://github.com/sbsahane12/Handwriting-digit-recognition-using-SVM.git) 
## Problem Statement
As people have different handwriting it is difficult for a computer or any device to understand those handwritings of different people. The handwriting recognition is the ability of a computer or a device to take input handwriting in the form of an image such as picture of handwritten text which is fed to the pattern recognition algorithm or a model
## Steps :
- Data Inspection â€“ Missing Values if any, EDA
- Outlier Analysis
- Data Pre-processing
- Finding Optimal number of Clusters
- Modelling
## Install
This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)
## Conclusion : 
- Therefore as we can see in this scenario the non Linear model provides a higher accuracy than the Linear model which are **94%** and **91%** respectively.  
- Therefore we can conclude that the problem is non linear in nature.



# [Project  8: Heart Deasis Prediction Using ML Alogorithm](https://github.com/sbsahane12/Machine-Learning-Projects-for-2023.git) 
## Problem Statement
We have a data which classified if patients have heart disease or not according to features in it. We will try to use this data to create a model which tries predict if a patient has this disease or not. We will use logistic regression (classification) algorithm.
## Steps :
- Data Inspection â€“ Missing Values if any, EDA
- Outlier Analysis
- Data Pre-processing
- Finding Optimal number of Clusters
- Modelling
- 
Data contains:
* age - age in years 
* sex - (1 = male; 0 = female)
* cp - chest pain type
* trestbps - resting blood pressure (in mm Hg on admission to the hospital) 
* chol - serum cholestoral in mg/dl
* fbs - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) 
* restecg - resting electrocardiographic results
* thalach - maximum heart rate achieved
* exang - exercise induced angina (1 = yes; 0 = no) 
* oldpeak - ST depression induced by exercise relative to rest
* slope - the slope of the peak exercise ST segment 
* ca - number of major vessels (0-3) colored by flourosopy
* thal - 3 = normal; 6 = fixed defect; 7 = reversable defect  
* target - have disease or not (1=yes, 0=no)
* 
## Install
This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)
## Conclusion : 
- Our models work fine but best of them are KNN and Random Forest with 88.52% of accuracy. Let's look their confusion matrixes.




# [Project 9:HR - Attrition Analytics -  Exploratory Analysis & Predictive Modeling Using Advanced Machine Learning](https://github.com/sbsahane12/HR-Attrition-Analytics-Exploratory-Analysis-Predictive-Modeling-Using-Advanced-ML)
## Aim :
> Human Resources are critical resources of any organiazation. Organizations spend huge amount of time and money to hire 
> and nuture their employees. It is a huge loss for companies if employees leave, especially the key resources. 
> So if HR can predict weather employees are at risk for leaving the company, it will allow them to identify the attrition 
> risks and help understand and provie necessary support to retain those employees or do preventive hiring to minimize the 
> impact to the orgranization.

## Install
This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)

You will also need to have software installed to run and execute a [Jupyter Notebook](http://ipython.org/notebook.html)

If you do not have Python installed yet, it is highly recommended that you install the [Anaconda](http://continuum.io/downloads) distribution of Python, which already has the above packages and more included.

### DATA ATRRIBUTES

- satisfaction_level: Employee satisfaction level <br>
- last_evaluation: Last evaluation  <br>
- number_project: Number of projects  <br>
- average_montly_hours: Average monthly hours <br>
- time_spend_company: Time spent at the company <br>
- Work_accident: Whether they have had a work accident <br>
- promotion_last_5years: Whether they have had a promotion in the last 5 years <br>
- department: Department <br>
- salary: Salary <br>
- left: Whether the employee has left <br>
## Models trained on: 
1. Linear Regression
2. Logistic Regression Algorithm 




# [Project  10:Lead scoring case study analysis using ML](https://github.com/sbsahane12/Lead-scoring-case-study-analysis-using-ML.git) 
## Problem Statement  
### Background  - Business Understanding   
- Building Logistic regression model & assigning Lead Scores to the prospective candidates of X Education
## Problem description
- X Education is an online Education company which has Lead database, some of which got converted & some didnâ€˜t
- The typical lead conversion rate is 30% which is expected to be maximized to atleast 80%
- Target is to identify the â€˜Hot Leads' which have a high conversion rate.
- The 'Hot Leadsâ€™ to be identified by cutoff Lead Scores Lead scores to be assigned to each candidates based on probabilities calculated by Logistic regression model

This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)
## Data Preparation  
- Data Inspection and Missing value treatment
- Dummy variable creation
- Logistic regression modelling
- Model Accuracy Check
- Model fit on testdata
- Conclusion
- Recommendations\

## Data Inspection and Missing value treatment
- Columns containing >70% missing data were dropped.
- â€˜Cityâ€™ column had ~40% missing values & was dropped
- In absence of any visible correlation with Activity & Profile, these columns were dropped too
- *Asymmetric Index columns were checked for any possible relation to impute missing values Other columns with possible imputations were handled appropriately

- Unique value columns : 
- Columns with only one type of unique values were dropped in absence of variability

- Imputation :
- High missing value containing columns were imputed with suitable values

## Model Building : 
- 15 Features were selected using RFE.
- Six Logistic regression models were built iteratively
- Final model was selected based on:
- p-values <0.05 for all variables, indicatingsignificance 2.VIF< 5, indicating absence of multicollinearity
- Model performancemeasures
- High values of Accuracy, Sensitivity & Specificity indicate good predictive powers of model.
- Low False positive rate indicates modelâ€™s ability to
- predict positive values accurately.
## Recommendations

ðŸ œToget more customers, XEducation must keep the lead score lower, starting at â€˜0â€™. But to achieve target conversion of greater than 80%, it should keep the cut off at 30.
ðŸ œThus, in the model, data frame changed for cut off Lead Score to gauge the Conversion percentages w.r.t. actual converted.
ðŸ œLowering the lead score cut off reduces conversion %, but it increases
number of actual converted.
ðŸ œBased on the man power availability with XEducation, it may decide to give weightage to conversion %oractual numbers.


# [Project 11: Student Mark Predictor Project Deployment Using Advanced Machine Learning](https://github.com/sbsahane12/Student-Mark-Predictor-Project-Deployment-Using-Advanced-Machine-Learning-.git)
## Install
This project requires Python 3.6 and the following libraries installed:
- [NumPy](http://www.numpy.org/)
- [Pandas](http://pandas.pydata.org)
- [matplotlib](http://matplotlib.org/)
- [scikit-learn](http://scikit-learn.org/stable/)
- [seaborn](https://seaborn.pydata.org/)

## Models trained on: 
1. Linear Regression Algorithm 
